{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load requiered modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from utils import TrainManager\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = 'CPU'\n",
    "MODEL_PATH = 'output_models_full/efficientnet_v2_s_512/epoch=4-EER_VER=8.65.ckpt'\n",
    "BACKBONE = 'efficientnet_v2_s'\n",
    "NUM_IDENTITIES = 11670\n",
    "NUM_FEATURE = 512\n",
    "INPUT_SIZE = 324\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                    transforms.Resize((INPUT_SIZE, INPUT_SIZE)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Openvino core, load the model and compile the latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OpenVINO Core object instance\n",
    "core = ov.Core()\n",
    "\n",
    "model = TrainManager.load_from_checkpoint(checkpoint_path=MODEL_PATH, model=BACKBONE, num_identities=NUM_IDENTITIES, num_features=NUM_FEATURE)\n",
    "model.eval()\n",
    "model.to('cpu')\n",
    "\n",
    "# Convert model to openvino.runtime.Model object\n",
    "ov_model = ov.convert_model(model.backbone, input=[[1,3,INPUT_SIZE,INPUT_SIZE]])\n",
    "\n",
    "compiled_model = core.compile_model(ov_model, device_name=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the input image and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 324, 324])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading input images for subject #1 and subject #2\n",
    "palm_s1 = Image.open('examples/subject1/palm/83d72e4f-ea43-417b-80a1-a3e7ba61b096.jpg')\n",
    "peace_s1 = Image.open('examples/subject1/peace/5105a824-0c49-41d9-b21b-0d3301918eb9.jpg')\n",
    "\n",
    "palm_s2 = Image.open('examples/subject2/palm/642321dd-5101-42f5-b974-4b9236f85fc7.jpg')\n",
    "peace_s2 = Image.open('examples/subject2/peace/231de2d7-f406-44fd-8c17-a3c8854da40b.jpg')\n",
    "\n",
    "#Transforming the images for both subjects\n",
    "palm_s1 = torch.unsqueeze(transform(palm_s1), dim=0)\n",
    "peace_s1 = torch.unsqueeze(transform(peace_s1), dim=0) \n",
    "\n",
    "palm_s2 = torch.unsqueeze(transform(palm_s2), dim=0)\n",
    "peace_s2 = torch.unsqueeze(transform(peace_s2), dim=0) \n",
    "\n",
    "palm_s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model inference for each subject's image and get the feature embedding\n",
    "palm_s1_emb = compiled_model(palm_s1)[0][0]\n",
    "palm_s1_emb = palm_s1_emb / np.linalg.norm(palm_s1_emb)\n",
    "\n",
    "peace_s1_emb = compiled_model(peace_s1)[0][0]\n",
    "peace_s1_emb = peace_s1_emb / np.linalg.norm(peace_s1_emb)\n",
    "\n",
    "palm_s2_emb = compiled_model(palm_s2)[0][0]\n",
    "palm_s2_emb = palm_s2_emb / np.linalg.norm(palm_s2_emb)\n",
    "\n",
    "peace_s2_emb = compiled_model(peace_s2)[0][0]\n",
    "peace_s2_emb = peace_s2_emb / np.linalg.norm(peace_s2_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the time spent to compute an embedding using static onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.3 ms ± 625 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "palm_s1_emb = compiled_model(palm_s1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the time spent to compute an embedding using the pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 ms ± 3.83 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "palm_s1_emb_pytorch = model.backbone(palm_s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether pytorch model and the openvino produce the same embedding\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between the same embeddings -> 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "palm_s1_emb_pytorch_np = F.normalize(palm_s1_emb_pytorch)\n",
    "palm_s1_emb_pytorch_np = palm_s1_emb_pytorch.detach().cpu().numpy()\n",
    "palm_s1_emb_pytorch_np = palm_s1_emb_pytorch_np[0]\n",
    "\n",
    "value_s1 = np.dot(palm_s1_emb,palm_s1_emb_pytorch_np)/(norm(palm_s1_emb)*norm(palm_s1_emb_pytorch_np))\n",
    "\n",
    "print('Similarity between the same embeddings -> {}'.format(value_s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute similarity between samples from the same subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between samples of subject 1 -> 0.9951640367507935\n",
      "Similarity between samples of subject 2 -> 0.9912264347076416\n"
     ]
    }
   ],
   "source": [
    "value_s1 = np.dot(palm_s1_emb,peace_s1_emb)/(norm(palm_s1_emb)*norm(peace_s1_emb))\n",
    "value_s2 = np.dot(palm_s2_emb,peace_s2_emb)/(norm(palm_s1_emb)*norm(peace_s2_emb))\n",
    "\n",
    "print('Similarity between samples of subject 1 -> {}'.format(value_s1))\n",
    "print('Similarity between samples of subject 2 -> {}'.format(value_s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute similarity between samples from the different subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between palm samples of different subjects -> 0.9408684372901917\n",
      "Similarity between peace samples of different subjects -> 0.94172602891922\n"
     ]
    }
   ],
   "source": [
    "value_palm_s1_2 = np.dot(palm_s1_emb,palm_s2_emb)/(norm(palm_s1_emb)*norm(palm_s2_emb))\n",
    "value_peace_s1_2 = np.dot(peace_s1_emb,peace_s2_emb)/(norm(peace_s1_emb)*norm(peace_s2_emb))\n",
    "\n",
    "print('Similarity between palm samples of different subjects -> {}'.format(value_palm_s1_2))\n",
    "print('Similarity between peace samples of different subjects -> {}'.format(value_peace_s1_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "STTN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
